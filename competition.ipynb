{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "competition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJvsOeXang15XoAKiqcPVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c81b6e92a6a4c14879eb68376074058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf1f6e6855334671a50921f449bcfb80",
              "IPY_MODEL_d0ffda08bc09404f9f47d44a87229245",
              "IPY_MODEL_411efb65eb004d57b1ccff9840ba8e6e"
            ],
            "layout": "IPY_MODEL_117f3863b4c346c0a4823d9deef4520d"
          }
        },
        "bf1f6e6855334671a50921f449bcfb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07b4210aa57c4d6199ec0191d5478c50",
            "placeholder": "​",
            "style": "IPY_MODEL_03b5d875c34342899d72b62b08392f9d",
            "value": "100%"
          }
        },
        "d0ffda08bc09404f9f47d44a87229245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f1f5d91942410fa5af9fe6e0786cd0",
            "max": 102502400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66772fd70fc645e79c172a675590b507",
            "value": 102502400
          }
        },
        "411efb65eb004d57b1ccff9840ba8e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c64e98b6bf504a9a93f33ac1f41498bf",
            "placeholder": "​",
            "style": "IPY_MODEL_5ef3a01a2e7d40af841a592fab9c9b5d",
            "value": " 97.8M/97.8M [00:04&lt;00:00, 37.8MB/s]"
          }
        },
        "117f3863b4c346c0a4823d9deef4520d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07b4210aa57c4d6199ec0191d5478c50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03b5d875c34342899d72b62b08392f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7f1f5d91942410fa5af9fe6e0786cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66772fd70fc645e79c172a675590b507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c64e98b6bf504a9a93f33ac1f41498bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ef3a01a2e7d40af841a592fab9c9b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BuddGao/DL-Competition/blob/main/competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HLmZdwaRm_oK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aeb1b8d-d444-423f-fe04-ffdab4e13780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## preprocess data\n",
        "## create custom data class\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "\n",
        "#from monai.transforms import Compose, LoadImage, AddChannel, ScaleIntensity,RandRotate, ToTensor, RandFlip, RandZoom, Resize, RandGaussianNoise\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models, datasets\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import glob\n",
        "#from torchsampler import ImbalancedDatasetSampler\n",
        "class CovidDataset(Dataset):\n",
        "    \"\"\"Covid CT dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.label_data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        ## you can apply custom transformation on the image for data augmentation\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, self.label_data.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
        "        \n",
        " \n",
        "        p = self.label_data.iloc[idx, 1]\n",
        "\n",
        "        subject_num = self.label_data.iloc[idx, 2]\n",
        "        \n",
        "\n",
        "        # should be only applied on image, not percentage or subject #\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        sample = {'image': image, 'percentage': p, 'subject': subject_num, 'img_name':img_name}\n",
        "        return sample"
      ],
      "metadata": {
        "id": "95O_gDz_qtRL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CovidTestDataset(Dataset):\n",
        "    \"\"\"Covid CT TEST dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_list = []\n",
        "        for filename in glob.glob(self.root_dir+\"/*.png\"): #assuming png\n",
        "          self.image_list.append(filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = self.image_list[idx]\n",
        "        image = io.imread(img_name)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
        "        image = torchvision.transforms.functional.to_tensor(image)\n",
        "\n",
        "        # should be only applied on image, not percentage or subject #\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "    \n",
        "        sample = {'image': image, 'img_name':img_name}\n",
        "        return sample"
      ],
      "metadata": {
        "id": "mLJWR3q3rDhV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "covid_dataset_train_val = CovidDataset(csv_file='drive/MyDrive/DL_Project/Train.csv',\n",
        "                                    root_dir='drive/MyDrive/DL_Project/Train'\n",
        "                                    ,transform = transforms.Compose([\n",
        "                                                                   \n",
        "                                                transforms.ToPILImage(),               \n",
        "                                                \n",
        "                                                transforms.RandomHorizontalFlip(),\n",
        "                                                transforms.RandomRotation(15),\n",
        "                                                \n",
        "                                                transforms.Resize((224, 224)),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "                                    \n",
        "                                    )\n",
        "\n",
        "covid_dataset_test = CovidTestDataset(root_dir='drive/MyDrive/DL_Project/Val',\n",
        "                                      transform = transforms.Compose([\n",
        "                                                                       \n",
        "                                                transforms.ToPILImage(), \n",
        "                                                                 \n",
        "                                                transforms.Resize((224, 224)),\n",
        "                                                \n",
        "                                                transforms.ToTensor(),\n",
        "                                                \n",
        "                                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "                                   \n",
        "                                    )"
      ],
      "metadata": {
        "id": "1_7VyeU-rHx3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the index of subjects for k-fold cross validation\n",
        "def generate_index(dat,k):\n",
        "    \n",
        "    Y = list(range(0,dat[-1]['subject']+1))\n",
        "    kf = KFold(n_splits=k, shuffle=True)\n",
        "    n = len(Y)\n",
        "    index = {'train_index':[],\n",
        "             'val_index':[]}\n",
        "\n",
        "    for train_index, val_index in kf.split(np.zeros(n), Y):\n",
        "        index['train_index'].append(train_index)\n",
        "        index['val_index'].append(val_index)\n",
        "        \n",
        "    \n",
        "    return pd.DataFrame(index['train_index']).T,pd.DataFrame(index['val_index']).T"
      ],
      "metadata": {
        "id": "Frop9y5KaDU_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get links form subjects to images\n",
        "dfl = pd.read_csv('drive/MyDrive/DL_Project/Train.csv')\n",
        "subtoimage = []\n",
        "le = []\n",
        "for sub in range(0,covid_dataset_train_val[-1]['subject']+1):\n",
        "  subtoimage.append(dfl[dfl['0'] == sub].index.tolist())\n",
        "  le.append(len(dfl[dfl['0'] == sub].index.tolist()))"
      ],
      "metadata": {
        "id": "6HvaOrrVs5sd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get images' index form subjects' index\n",
        "def get_image_index(subindex,subtoimage):\n",
        "    imageindex = []\n",
        "    for sub in range(0,len(subindex)):\n",
        "        imageindex.extend(subtoimage[subindex[sub]])\n",
        "    return imageindex"
      ],
      "metadata": {
        "id": "UatrtHxvxXrG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels"
      ],
      "metadata": {
        "id": "T_AOEe-ycaue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c08c7a-af0e-4db9-d715-de237042df35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretrainedmodels\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (0.11.1+cu111)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (4.63.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (7.1.2)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=70c15db30314d14311718c0e382abe8dfd7766fb20b26d3c229afee640d9ebb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "sD8JtRvlcbxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c32a524-152e-4816-ece2-747f51cd8a2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pretrainedmodels\n",
        "#For model building\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CNN1(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(CNN1, self).__init__()\n",
        "        if pretrained is True:\n",
        "            self.model = pretrainedmodels.__dict__[\"resnet50\"](pretrained=\"imagenet\")\n",
        "        else:\n",
        "            self.model = pretrainedmodels.__dict__[\"resnet50\"](pretrained=None)\n",
        "        #self.fc1 = nn.Linear(512, 256)\n",
        "        self.conv1 = nn.Conv2d(2048, 512, (2,2), stride=1, padding =0)\n",
        "        self.conv2 = nn.Conv2d(512, 128, (2,2), stride=1, padding =0)\n",
        "        #self.pool = nn.AdaptiveAvgPool2d(output_size = (1,1))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.conv3 = nn.Conv2d(128, 32, (2,2), stride=1, padding =0)\n",
        "        self.conv4 = nn.Conv2d(32, 8, (2,2), stride=1, padding =0)\n",
        "        self.fc1 = nn.Linear(72, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        bs, _, _, _ = x.shape\n",
        "        x = self.model.features(x)\n",
        "        #x = F.adaptive_avg_pool2d(x, x.shape[3])\n",
        "        # \n",
        "        #label = self.fc1(x.reshape(bs, -1))\n",
        "        #x = self.pool(x)\n",
        "        #x = F.relu(self.fc1(x))\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv4(F.relu(x))\n",
        "        #print(x.size()) # [8, 8, 4, 4]\n",
        "        label = self.fc1(x.reshape(bs, -1))  # [8, 128]\n",
        "        return label"
      ],
      "metadata": {
        "id": "hTCqAlwLcb59"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "model_CNN = CNN1(True).to(device)\n",
        "\n",
        "#summary(model_CNN,input_size=(3, 224, 224))"
      ],
      "metadata": {
        "id": "PPUTngYAx7zT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4c81b6e92a6a4c14879eb68376074058",
            "bf1f6e6855334671a50921f449bcfb80",
            "d0ffda08bc09404f9f47d44a87229245",
            "411efb65eb004d57b1ccff9840ba8e6e",
            "117f3863b4c346c0a4823d9deef4520d",
            "07b4210aa57c4d6199ec0191d5478c50",
            "03b5d875c34342899d72b62b08392f9d",
            "d7f1f5d91942410fa5af9fe6e0786cd0",
            "66772fd70fc645e79c172a675590b507",
            "c64e98b6bf504a9a93f33ac1f41498bf",
            "5ef3a01a2e7d40af841a592fab9c9b5d"
          ]
        },
        "outputId": "6d676363-6112-45c5-9ea1-3065fac7d586"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c81b6e92a6a4c14879eb68376074058"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer,split_i, n_epochs=21):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf\n",
        "    model_best = model\n",
        "    for epoch in range(1, n_epochs):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        mse = 0\n",
        "        mymse = 0\n",
        "        # train the model #\n",
        "        model.train()\n",
        "        for batch_idx, sample_batched in enumerate(train_dataloader):\n",
        "            # importing data and moving to GPU\n",
        "            image, label = sample_batched['image'].to(device), sample_batched['percentage'].to(device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            output=model(image).reshape(-1)\n",
        "            # calculate loss\n",
        "            loss = criterion((output).type(torch.FloatTensor).to(device), label.type(torch.FloatTensor).to(device))\n",
        "            # back prop\n",
        "            loss.backward()\n",
        "            # grad\n",
        "            optimizer.step()\n",
        "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "            if batch_idx % 100 == 0:\n",
        "                print('Epoch %d, Batch %d loss: %.6f' %\n",
        "                  (epoch, batch_idx + 1, train_loss))\n",
        "        # validate the model #\n",
        "        model.eval()\n",
        "        \n",
        "        for batch_idx, sample_batched in enumerate(val_dataloader):\n",
        "            image, label = sample_batched['image'].to(device), sample_batched['percentage'].to(device)  \n",
        "            output = model(image).reshape(-1)\n",
        "            # calculate loss\n",
        "            loss=criterion((output).type(torch.FloatTensor).to(device), label.type(torch.FloatTensor).to(device))\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
        "\n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, train_loss, valid_loss))\n",
        "        \n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if valid_loss < valid_loss_min:\n",
        "            model_best = model\n",
        "            torch.save(model.state_dict(), 'drive/MyDrive/DL_Project/'+str(split_i)+'model.pth')\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "            valid_loss_min = valid_loss\n",
        "    \n",
        "\n",
        "    # return trained model\n",
        "    return model_best,valid_loss_min"
      ],
      "metadata": {
        "id": "U8Xtqd2rcihx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_dataloader(split_i, train_index, val_index, covid_dataset_train_val, subtoimage):\n",
        "  train_imageindex = get_image_index(train_index.iloc[:,split_i],subtoimage)\n",
        "  val_imageindex = get_image_index(val_index.iloc[:,split_i],subtoimage)\n",
        "  train_dataset = torch.utils.data.Subset(covid_dataset_train_val,train_imageindex)\n",
        "  val_dataset = torch.utils.data.Subset(covid_dataset_train_val,val_imageindex)\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "  return train_dataloader,val_dataloader"
      ],
      "metadata": {
        "id": "MKZM4qOkuewr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 4\n",
        "train_index,val_index = generate_index(covid_dataset_train_val,k)\n",
        "test_dataloader = DataLoader(covid_dataset_test, batch_size=8, shuffle=True, num_workers=0)\n",
        "\n",
        "valid_cvloss = []\n",
        "test_df = []\n",
        "mae_dl = []\n",
        "mamse = []\n",
        "for split_i in range(0,k):\n",
        "  ## split given train set to train & val set by subjects\n",
        "\n",
        "  train_dataloader,val_dataloader = train_val_dataloader(split_i, train_index, val_index, covid_dataset_train_val, subtoimage)\n",
        "  #Setting model and moving to device\n",
        "  model_CNN = CNN1(True).to(device)\n",
        "\n",
        "  # Freeze training for all \"features\" layers\n",
        "  for param in model_CNN.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  model_CNN.conv1 = nn.Conv2d(2048, 512, (2,2), stride=1, padding =0)\n",
        "  model_CNN.conv2 = nn.Conv2d(512, 128, (2,2), stride=1, padding =0)\n",
        "  model_CNN.conv3 = nn.Conv2d(128, 32, (2,2), stride=1, padding =0)\n",
        "  model_CNN.conv4 = nn.Conv2d(32, 8, (2,2), stride=1, padding =0)\n",
        "  model_CNN.fc1 = nn.Linear(72, 1)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      model_CNN = model_CNN.cuda()\n",
        "  criterion = nn.L1Loss()\n",
        "  \n",
        "  model_CNN_grad_paramaters = filter(lambda p: p.requires_grad, model_CNN.parameters())\n",
        "  #optimizer = optim.SGD(model_CNN_grad_paramaters, lr=0.0005, momentum=0.9)\n",
        "  optimizer = optim.Adam(model_CNN_grad_paramaters, lr=0.0005)\n",
        "\n",
        "  ## training model\n",
        "  model_conv,valid_loss_min=train_model(model_CNN, criterion, optimizer,split_i)\n",
        "\n",
        "  valid_cvloss.append(valid_loss_min)\n",
        "\n",
        "  ## test_pred\n",
        "  \n",
        "  df = pd.DataFrame(columns=['image_name','output'])\n",
        "  for batch_idx, sample_batched in enumerate(test_dataloader):\n",
        "     image= sample_batched['image'].to(device)\n",
        "     img_name= sample_batched['img_name']\n",
        "     output = model_conv(image).type(torch.LongTensor).reshape(-1)\n",
        "     img_name = np.array(img_name).reshape(output.shape[0],1)\n",
        "     o = output.cpu().data.numpy().reshape(output.shape[0],1)\n",
        "     a = np.concatenate((img_name,o),axis=1)\n",
        "     df = df.append(pd.DataFrame(a, columns=df.columns), ignore_index=True)\n",
        "  df.to_csv('drive/MyDrive/DL_Project/' + str(split_i) + '4fcv.csv', index=False, header=False)  \n",
        "  test_df.append(df)\n"
      ],
      "metadata": {
        "id": "zE5E7Cu8cGio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4861c9-007f-4c7d-99e9-3d10195da193"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 1 loss: 4.253921\n",
            "Epoch 1, Batch 101 loss: 13.196488\n",
            "Epoch 1, Batch 201 loss: 12.439745\n",
            "Epoch: 1 \tTraining Loss: 11.913354 \tValidation Loss: 20.650583\n",
            "Validation loss decreased (inf --> 20.650583).  Saving model ...\n",
            "Epoch 2, Batch 1 loss: 23.961227\n",
            "Epoch 2, Batch 101 loss: 10.539352\n",
            "Epoch 2, Batch 201 loss: 9.579531\n",
            "Epoch: 2 \tTraining Loss: 9.753479 \tValidation Loss: 12.357438\n",
            "Validation loss decreased (20.650583 --> 12.357438).  Saving model ...\n",
            "Epoch 3, Batch 1 loss: 17.224779\n",
            "Epoch 3, Batch 101 loss: 9.101274\n",
            "Epoch 3, Batch 201 loss: 9.174733\n",
            "Epoch: 3 \tTraining Loss: 8.892602 \tValidation Loss: 10.877577\n",
            "Validation loss decreased (12.357438 --> 10.877577).  Saving model ...\n",
            "Epoch 4, Batch 1 loss: 3.407007\n",
            "Epoch 4, Batch 101 loss: 8.466953\n",
            "Epoch 4, Batch 201 loss: 8.519751\n",
            "Epoch: 4 \tTraining Loss: 8.236672 \tValidation Loss: 12.814761\n",
            "Epoch 5, Batch 1 loss: 5.645549\n",
            "Epoch 5, Batch 101 loss: 7.763704\n",
            "Epoch 5, Batch 201 loss: 7.686499\n",
            "Epoch: 5 \tTraining Loss: 7.638631 \tValidation Loss: 11.946342\n",
            "Epoch 6, Batch 1 loss: 9.670622\n",
            "Epoch 6, Batch 101 loss: 7.906880\n",
            "Epoch 6, Batch 201 loss: 7.503051\n",
            "Epoch: 6 \tTraining Loss: 7.470507 \tValidation Loss: 10.068926\n",
            "Validation loss decreased (10.877577 --> 10.068926).  Saving model ...\n",
            "Epoch 7, Batch 1 loss: 8.875170\n",
            "Epoch 7, Batch 101 loss: 7.069515\n",
            "Epoch 7, Batch 201 loss: 7.527364\n",
            "Epoch: 7 \tTraining Loss: 7.397370 \tValidation Loss: 11.426791\n",
            "Epoch 8, Batch 1 loss: 11.951466\n",
            "Epoch 8, Batch 101 loss: 7.260641\n",
            "Epoch 8, Batch 201 loss: 7.349358\n",
            "Epoch: 8 \tTraining Loss: 7.325282 \tValidation Loss: 11.389808\n",
            "Epoch 9, Batch 1 loss: 7.264288\n",
            "Epoch 9, Batch 101 loss: 6.650217\n",
            "Epoch 9, Batch 201 loss: 6.809863\n",
            "Epoch: 9 \tTraining Loss: 6.976051 \tValidation Loss: 11.965923\n",
            "Epoch 10, Batch 1 loss: 5.130805\n",
            "Epoch 10, Batch 101 loss: 6.697167\n",
            "Epoch 10, Batch 201 loss: 6.793514\n",
            "Epoch: 10 \tTraining Loss: 6.823997 \tValidation Loss: 9.663274\n",
            "Validation loss decreased (10.068926 --> 9.663274).  Saving model ...\n",
            "Epoch 11, Batch 1 loss: 13.204384\n",
            "Epoch 11, Batch 101 loss: 6.544528\n",
            "Epoch 11, Batch 201 loss: 6.725343\n",
            "Epoch: 11 \tTraining Loss: 6.638562 \tValidation Loss: 9.784491\n",
            "Epoch 12, Batch 1 loss: 16.871264\n",
            "Epoch 12, Batch 101 loss: 7.034800\n",
            "Epoch 12, Batch 201 loss: 6.840103\n",
            "Epoch: 12 \tTraining Loss: 6.702440 \tValidation Loss: 9.464392\n",
            "Validation loss decreased (9.663274 --> 9.464392).  Saving model ...\n",
            "Epoch 13, Batch 1 loss: 4.021716\n",
            "Epoch 13, Batch 101 loss: 6.563895\n",
            "Epoch 13, Batch 201 loss: 6.661081\n",
            "Epoch: 13 \tTraining Loss: 6.528984 \tValidation Loss: 10.804790\n",
            "Epoch 14, Batch 1 loss: 2.477664\n",
            "Epoch 14, Batch 101 loss: 6.789059\n",
            "Epoch 14, Batch 201 loss: 6.579140\n",
            "Epoch: 14 \tTraining Loss: 6.580997 \tValidation Loss: 9.959090\n",
            "Epoch 15, Batch 1 loss: 5.448544\n",
            "Epoch 15, Batch 101 loss: 6.239489\n",
            "Epoch 15, Batch 201 loss: 6.505679\n",
            "Epoch: 15 \tTraining Loss: 6.442430 \tValidation Loss: 10.396919\n",
            "Epoch 16, Batch 1 loss: 8.681973\n",
            "Epoch 16, Batch 101 loss: 6.303425\n",
            "Epoch 16, Batch 201 loss: 6.357510\n",
            "Epoch: 16 \tTraining Loss: 6.249971 \tValidation Loss: 9.595557\n",
            "Epoch 17, Batch 1 loss: 10.667195\n",
            "Epoch 17, Batch 101 loss: 6.258428\n",
            "Epoch 17, Batch 201 loss: 6.346763\n",
            "Epoch: 17 \tTraining Loss: 6.265134 \tValidation Loss: 10.465786\n",
            "Epoch 18, Batch 1 loss: 13.566210\n",
            "Epoch 18, Batch 101 loss: 6.658900\n",
            "Epoch 18, Batch 201 loss: 6.242529\n",
            "Epoch: 18 \tTraining Loss: 6.037469 \tValidation Loss: 10.478824\n",
            "Epoch 19, Batch 1 loss: 8.459280\n",
            "Epoch 19, Batch 101 loss: 6.199288\n",
            "Epoch 19, Batch 201 loss: 5.949212\n",
            "Epoch: 19 \tTraining Loss: 6.093674 \tValidation Loss: 11.025709\n",
            "Epoch 20, Batch 1 loss: 12.256685\n",
            "Epoch 20, Batch 101 loss: 5.921412\n",
            "Epoch 20, Batch 201 loss: 5.917892\n",
            "Epoch: 20 \tTraining Loss: 5.852747 \tValidation Loss: 10.198168\n",
            "Epoch 1, Batch 1 loss: 28.501831\n",
            "Epoch 1, Batch 101 loss: 16.145798\n",
            "Epoch 1, Batch 201 loss: 13.821934\n",
            "Epoch: 1 \tTraining Loss: 13.229631 \tValidation Loss: 9.142482\n",
            "Validation loss decreased (inf --> 9.142482).  Saving model ...\n",
            "Epoch 2, Batch 1 loss: 5.946219\n",
            "Epoch 2, Batch 101 loss: 10.656434\n",
            "Epoch 2, Batch 201 loss: 10.018896\n",
            "Epoch: 2 \tTraining Loss: 10.165725 \tValidation Loss: 7.908441\n",
            "Validation loss decreased (9.142482 --> 7.908441).  Saving model ...\n",
            "Epoch 3, Batch 1 loss: 5.486751\n",
            "Epoch 3, Batch 101 loss: 9.599090\n",
            "Epoch 3, Batch 201 loss: 9.776984\n",
            "Epoch: 3 \tTraining Loss: 9.757039 \tValidation Loss: 7.657620\n",
            "Validation loss decreased (7.908441 --> 7.657620).  Saving model ...\n",
            "Epoch 4, Batch 1 loss: 3.393152\n",
            "Epoch 4, Batch 101 loss: 9.186603\n",
            "Epoch 4, Batch 201 loss: 9.441341\n",
            "Epoch: 4 \tTraining Loss: 9.250737 \tValidation Loss: 7.378240\n",
            "Validation loss decreased (7.657620 --> 7.378240).  Saving model ...\n",
            "Epoch 5, Batch 1 loss: 13.951009\n",
            "Epoch 5, Batch 101 loss: 8.831049\n",
            "Epoch 5, Batch 201 loss: 9.074837\n",
            "Epoch: 5 \tTraining Loss: 8.918516 \tValidation Loss: 6.549767\n",
            "Validation loss decreased (7.378240 --> 6.549767).  Saving model ...\n",
            "Epoch 6, Batch 1 loss: 2.834842\n",
            "Epoch 6, Batch 101 loss: 7.802176\n",
            "Epoch 6, Batch 201 loss: 8.112582\n",
            "Epoch: 6 \tTraining Loss: 8.501801 \tValidation Loss: 7.215508\n",
            "Epoch 7, Batch 1 loss: 12.269807\n",
            "Epoch 7, Batch 101 loss: 8.071588\n",
            "Epoch 7, Batch 201 loss: 8.297384\n",
            "Epoch: 7 \tTraining Loss: 8.275161 \tValidation Loss: 7.298892\n",
            "Epoch 8, Batch 1 loss: 6.537021\n",
            "Epoch 8, Batch 101 loss: 8.561845\n",
            "Epoch 8, Batch 201 loss: 8.109641\n",
            "Epoch: 8 \tTraining Loss: 8.026250 \tValidation Loss: 7.155709\n",
            "Epoch 9, Batch 1 loss: 18.724483\n",
            "Epoch 9, Batch 101 loss: 8.021960\n",
            "Epoch 9, Batch 201 loss: 7.848445\n",
            "Epoch: 9 \tTraining Loss: 7.717522 \tValidation Loss: 6.439145\n",
            "Validation loss decreased (6.549767 --> 6.439145).  Saving model ...\n",
            "Epoch 10, Batch 1 loss: 9.967537\n",
            "Epoch 10, Batch 101 loss: 8.162153\n",
            "Epoch 10, Batch 201 loss: 8.246552\n",
            "Epoch: 10 \tTraining Loss: 8.027729 \tValidation Loss: 6.504558\n",
            "Epoch 11, Batch 1 loss: 5.068777\n",
            "Epoch 11, Batch 101 loss: 7.823337\n",
            "Epoch 11, Batch 201 loss: 7.865331\n",
            "Epoch: 11 \tTraining Loss: 7.768388 \tValidation Loss: 6.916297\n",
            "Epoch 12, Batch 1 loss: 4.944293\n",
            "Epoch 12, Batch 101 loss: 7.263529\n",
            "Epoch 12, Batch 201 loss: 7.266934\n",
            "Epoch: 12 \tTraining Loss: 7.334814 \tValidation Loss: 6.421615\n",
            "Validation loss decreased (6.439145 --> 6.421615).  Saving model ...\n",
            "Epoch 13, Batch 1 loss: 2.417886\n",
            "Epoch 13, Batch 101 loss: 7.244456\n",
            "Epoch 13, Batch 201 loss: 7.064428\n",
            "Epoch: 13 \tTraining Loss: 7.158454 \tValidation Loss: 6.509346\n",
            "Epoch 14, Batch 1 loss: 3.584051\n",
            "Epoch 14, Batch 101 loss: 6.736976\n",
            "Epoch 14, Batch 201 loss: 7.180482\n",
            "Epoch: 14 \tTraining Loss: 7.282482 \tValidation Loss: 6.787421\n",
            "Epoch 15, Batch 1 loss: 5.552626\n",
            "Epoch 15, Batch 101 loss: 7.839609\n",
            "Epoch 15, Batch 201 loss: 7.405638\n",
            "Epoch: 15 \tTraining Loss: 7.427282 \tValidation Loss: 6.280333\n",
            "Validation loss decreased (6.421615 --> 6.280333).  Saving model ...\n",
            "Epoch 16, Batch 1 loss: 11.215418\n",
            "Epoch 16, Batch 101 loss: 6.900133\n",
            "Epoch 16, Batch 201 loss: 6.882006\n",
            "Epoch: 16 \tTraining Loss: 7.014538 \tValidation Loss: 6.272706\n",
            "Validation loss decreased (6.280333 --> 6.272706).  Saving model ...\n",
            "Epoch 17, Batch 1 loss: 7.237701\n",
            "Epoch 17, Batch 101 loss: 7.172211\n",
            "Epoch 17, Batch 201 loss: 7.165118\n",
            "Epoch: 17 \tTraining Loss: 7.055444 \tValidation Loss: 6.616180\n",
            "Epoch 18, Batch 1 loss: 8.592787\n",
            "Epoch 18, Batch 101 loss: 6.822843\n",
            "Epoch 18, Batch 201 loss: 6.734110\n",
            "Epoch: 18 \tTraining Loss: 6.876549 \tValidation Loss: 7.099759\n",
            "Epoch 19, Batch 1 loss: 6.354923\n",
            "Epoch 19, Batch 101 loss: 6.701952\n",
            "Epoch 19, Batch 201 loss: 6.561120\n",
            "Epoch: 19 \tTraining Loss: 6.926091 \tValidation Loss: 6.497973\n",
            "Epoch 20, Batch 1 loss: 12.519465\n",
            "Epoch 20, Batch 101 loss: 6.603985\n",
            "Epoch 20, Batch 201 loss: 6.715095\n",
            "Epoch: 20 \tTraining Loss: 6.714531 \tValidation Loss: 6.926375\n",
            "Epoch 1, Batch 1 loss: 24.214893\n",
            "Epoch 1, Batch 101 loss: 15.281376\n",
            "Epoch 1, Batch 201 loss: 13.756602\n",
            "Epoch: 1 \tTraining Loss: 13.018680 \tValidation Loss: 9.927131\n",
            "Validation loss decreased (inf --> 9.927131).  Saving model ...\n",
            "Epoch 2, Batch 1 loss: 6.563417\n",
            "Epoch 2, Batch 101 loss: 10.331760\n",
            "Epoch 2, Batch 201 loss: 10.191121\n",
            "Epoch: 2 \tTraining Loss: 10.112159 \tValidation Loss: 8.163367\n",
            "Validation loss decreased (9.927131 --> 8.163367).  Saving model ...\n",
            "Epoch 3, Batch 1 loss: 3.870850\n",
            "Epoch 3, Batch 101 loss: 8.722121\n",
            "Epoch 3, Batch 201 loss: 8.918482\n",
            "Epoch: 3 \tTraining Loss: 9.142190 \tValidation Loss: 7.999688\n",
            "Validation loss decreased (8.163367 --> 7.999688).  Saving model ...\n",
            "Epoch 4, Batch 1 loss: 5.562275\n",
            "Epoch 4, Batch 101 loss: 8.986445\n",
            "Epoch 4, Batch 201 loss: 8.678145\n",
            "Epoch: 4 \tTraining Loss: 8.787946 \tValidation Loss: 7.351040\n",
            "Validation loss decreased (7.999688 --> 7.351040).  Saving model ...\n",
            "Epoch 5, Batch 1 loss: 6.846584\n",
            "Epoch 5, Batch 101 loss: 8.833721\n",
            "Epoch 5, Batch 201 loss: 8.462426\n",
            "Epoch: 5 \tTraining Loss: 8.436401 \tValidation Loss: 7.218036\n",
            "Validation loss decreased (7.351040 --> 7.218036).  Saving model ...\n",
            "Epoch 6, Batch 1 loss: 12.406462\n",
            "Epoch 6, Batch 101 loss: 7.799367\n",
            "Epoch 6, Batch 201 loss: 8.072537\n",
            "Epoch: 6 \tTraining Loss: 7.976831 \tValidation Loss: 8.255823\n",
            "Epoch 7, Batch 1 loss: 4.334257\n",
            "Epoch 7, Batch 101 loss: 8.008424\n",
            "Epoch 7, Batch 201 loss: 7.752194\n",
            "Epoch: 7 \tTraining Loss: 7.862155 \tValidation Loss: 8.730456\n",
            "Epoch 8, Batch 1 loss: 14.051873\n",
            "Epoch 8, Batch 101 loss: 7.720382\n",
            "Epoch 8, Batch 201 loss: 7.365367\n",
            "Epoch: 8 \tTraining Loss: 7.336794 \tValidation Loss: 7.295314\n",
            "Epoch 9, Batch 1 loss: 6.582391\n",
            "Epoch 9, Batch 101 loss: 8.087302\n",
            "Epoch 9, Batch 201 loss: 7.922912\n",
            "Epoch: 9 \tTraining Loss: 7.821318 \tValidation Loss: 7.134693\n",
            "Validation loss decreased (7.218036 --> 7.134693).  Saving model ...\n",
            "Epoch 10, Batch 1 loss: 12.792186\n",
            "Epoch 10, Batch 101 loss: 7.420733\n",
            "Epoch 10, Batch 201 loss: 7.147787\n",
            "Epoch: 10 \tTraining Loss: 7.229187 \tValidation Loss: 8.053275\n",
            "Epoch 11, Batch 1 loss: 7.637383\n",
            "Epoch 11, Batch 101 loss: 6.732385\n",
            "Epoch 11, Batch 201 loss: 6.778472\n",
            "Epoch: 11 \tTraining Loss: 6.803595 \tValidation Loss: 7.269682\n",
            "Epoch 12, Batch 1 loss: 5.826945\n",
            "Epoch 12, Batch 101 loss: 7.442153\n",
            "Epoch 12, Batch 201 loss: 6.911563\n",
            "Epoch: 12 \tTraining Loss: 6.854887 \tValidation Loss: 7.689487\n",
            "Epoch 13, Batch 1 loss: 6.145434\n",
            "Epoch 13, Batch 101 loss: 7.429439\n",
            "Epoch 13, Batch 201 loss: 7.037255\n",
            "Epoch: 13 \tTraining Loss: 6.963965 \tValidation Loss: 6.776065\n",
            "Validation loss decreased (7.134693 --> 6.776065).  Saving model ...\n",
            "Epoch 14, Batch 1 loss: 5.811404\n",
            "Epoch 14, Batch 101 loss: 7.175637\n",
            "Epoch 14, Batch 201 loss: 7.088198\n",
            "Epoch: 14 \tTraining Loss: 6.972094 \tValidation Loss: 8.354400\n",
            "Epoch 15, Batch 1 loss: 9.427396\n",
            "Epoch 15, Batch 101 loss: 6.929965\n",
            "Epoch 15, Batch 201 loss: 6.598483\n",
            "Epoch: 15 \tTraining Loss: 6.817784 \tValidation Loss: 8.706361\n",
            "Epoch 16, Batch 1 loss: 7.450696\n",
            "Epoch 16, Batch 101 loss: 6.879435\n",
            "Epoch 16, Batch 201 loss: 6.509475\n",
            "Epoch: 16 \tTraining Loss: 6.654000 \tValidation Loss: 7.764827\n",
            "Epoch 17, Batch 1 loss: 1.918927\n",
            "Epoch 17, Batch 101 loss: 6.704721\n",
            "Epoch 17, Batch 201 loss: 6.572192\n",
            "Epoch: 17 \tTraining Loss: 6.670605 \tValidation Loss: 7.184294\n",
            "Epoch 18, Batch 1 loss: 3.557575\n",
            "Epoch 18, Batch 101 loss: 6.446845\n",
            "Epoch 18, Batch 201 loss: 6.369343\n",
            "Epoch: 18 \tTraining Loss: 6.503150 \tValidation Loss: 7.405352\n",
            "Epoch 19, Batch 1 loss: 5.382568\n",
            "Epoch 19, Batch 101 loss: 6.533359\n",
            "Epoch 19, Batch 201 loss: 6.799722\n",
            "Epoch: 19 \tTraining Loss: 6.728631 \tValidation Loss: 7.607395\n",
            "Epoch 20, Batch 1 loss: 7.655067\n",
            "Epoch 20, Batch 101 loss: 6.314477\n",
            "Epoch 20, Batch 201 loss: 6.511628\n",
            "Epoch: 20 \tTraining Loss: 6.322021 \tValidation Loss: 7.466898\n",
            "Epoch 1, Batch 1 loss: 15.699725\n",
            "Epoch 1, Batch 101 loss: 15.554431\n",
            "Epoch 1, Batch 201 loss: 13.710147\n",
            "Epoch: 1 \tTraining Loss: 12.941256 \tValidation Loss: 9.447794\n",
            "Validation loss decreased (inf --> 9.447794).  Saving model ...\n",
            "Epoch 2, Batch 1 loss: 6.400432\n",
            "Epoch 2, Batch 101 loss: 10.661719\n",
            "Epoch 2, Batch 201 loss: 10.092724\n",
            "Epoch: 2 \tTraining Loss: 9.877405 \tValidation Loss: 8.884859\n",
            "Validation loss decreased (9.447794 --> 8.884859).  Saving model ...\n",
            "Epoch 3, Batch 1 loss: 4.039434\n",
            "Epoch 3, Batch 101 loss: 9.297326\n",
            "Epoch 3, Batch 201 loss: 9.466659\n",
            "Epoch: 3 \tTraining Loss: 9.678824 \tValidation Loss: 8.571818\n",
            "Validation loss decreased (8.884859 --> 8.571818).  Saving model ...\n",
            "Epoch 4, Batch 1 loss: 13.544397\n",
            "Epoch 4, Batch 101 loss: 8.932027\n",
            "Epoch 4, Batch 201 loss: 8.782701\n",
            "Epoch: 4 \tTraining Loss: 8.523894 \tValidation Loss: 9.286920\n",
            "Epoch 5, Batch 1 loss: 4.906716\n",
            "Epoch 5, Batch 101 loss: 8.016623\n",
            "Epoch 5, Batch 201 loss: 8.036704\n",
            "Epoch: 5 \tTraining Loss: 7.994548 \tValidation Loss: 8.112430\n",
            "Validation loss decreased (8.571818 --> 8.112430).  Saving model ...\n",
            "Epoch 6, Batch 1 loss: 3.841226\n",
            "Epoch 6, Batch 101 loss: 7.780711\n",
            "Epoch 6, Batch 201 loss: 7.809470\n",
            "Epoch: 6 \tTraining Loss: 7.673656 \tValidation Loss: 8.601172\n",
            "Epoch 7, Batch 1 loss: 2.541881\n",
            "Epoch 7, Batch 101 loss: 7.811511\n",
            "Epoch 7, Batch 201 loss: 7.451560\n",
            "Epoch: 7 \tTraining Loss: 7.792837 \tValidation Loss: 7.894596\n",
            "Validation loss decreased (8.112430 --> 7.894596).  Saving model ...\n",
            "Epoch 8, Batch 1 loss: 8.364882\n",
            "Epoch 8, Batch 101 loss: 7.536910\n",
            "Epoch 8, Batch 201 loss: 7.382914\n",
            "Epoch: 8 \tTraining Loss: 7.408349 \tValidation Loss: 8.792835\n",
            "Epoch 9, Batch 1 loss: 6.797365\n",
            "Epoch 9, Batch 101 loss: 7.840521\n",
            "Epoch 9, Batch 201 loss: 7.669792\n",
            "Epoch: 9 \tTraining Loss: 7.493992 \tValidation Loss: 8.040547\n",
            "Epoch 10, Batch 1 loss: 1.908578\n",
            "Epoch 10, Batch 101 loss: 6.495520\n",
            "Epoch 10, Batch 201 loss: 6.804486\n",
            "Epoch: 10 \tTraining Loss: 7.053340 \tValidation Loss: 8.114627\n",
            "Epoch 11, Batch 1 loss: 6.598236\n",
            "Epoch 11, Batch 101 loss: 6.879565\n",
            "Epoch 11, Batch 201 loss: 6.894477\n",
            "Epoch: 11 \tTraining Loss: 6.887797 \tValidation Loss: 8.677152\n",
            "Epoch 12, Batch 1 loss: 1.859594\n",
            "Epoch 12, Batch 101 loss: 7.227589\n",
            "Epoch 12, Batch 201 loss: 7.071633\n",
            "Epoch: 12 \tTraining Loss: 6.995079 \tValidation Loss: 7.838687\n",
            "Validation loss decreased (7.894596 --> 7.838687).  Saving model ...\n",
            "Epoch 13, Batch 1 loss: 4.564941\n",
            "Epoch 13, Batch 101 loss: 6.425257\n",
            "Epoch 13, Batch 201 loss: 6.669329\n",
            "Epoch: 13 \tTraining Loss: 6.884858 \tValidation Loss: 8.263968\n",
            "Epoch 14, Batch 1 loss: 4.929712\n",
            "Epoch 14, Batch 101 loss: 7.084768\n",
            "Epoch 14, Batch 201 loss: 6.822444\n",
            "Epoch: 14 \tTraining Loss: 6.868334 \tValidation Loss: 8.826439\n",
            "Epoch 15, Batch 1 loss: 15.043363\n",
            "Epoch 15, Batch 101 loss: 6.533258\n",
            "Epoch 15, Batch 201 loss: 6.711394\n",
            "Epoch: 15 \tTraining Loss: 6.868794 \tValidation Loss: 8.028435\n",
            "Epoch 16, Batch 1 loss: 13.617458\n",
            "Epoch 16, Batch 101 loss: 6.838938\n",
            "Epoch 16, Batch 201 loss: 6.875535\n",
            "Epoch: 16 \tTraining Loss: 6.853996 \tValidation Loss: 7.759648\n",
            "Validation loss decreased (7.838687 --> 7.759648).  Saving model ...\n",
            "Epoch 17, Batch 1 loss: 7.263775\n",
            "Epoch 17, Batch 101 loss: 6.339067\n",
            "Epoch 17, Batch 201 loss: 6.513809\n",
            "Epoch: 17 \tTraining Loss: 6.680613 \tValidation Loss: 7.490039\n",
            "Validation loss decreased (7.759648 --> 7.490039).  Saving model ...\n",
            "Epoch 18, Batch 1 loss: 3.953886\n",
            "Epoch 18, Batch 101 loss: 6.334199\n",
            "Epoch 18, Batch 201 loss: 6.304557\n",
            "Epoch: 18 \tTraining Loss: 6.440910 \tValidation Loss: 8.038506\n",
            "Epoch 19, Batch 1 loss: 6.777519\n",
            "Epoch 19, Batch 101 loss: 6.420020\n",
            "Epoch 19, Batch 201 loss: 6.726250\n",
            "Epoch: 19 \tTraining Loss: 6.700465 \tValidation Loss: 7.703415\n",
            "Epoch 20, Batch 1 loss: 10.299433\n",
            "Epoch 20, Batch 101 loss: 6.185072\n",
            "Epoch 20, Batch 201 loss: 6.229775\n",
            "Epoch: 20 \tTraining Loss: 6.423955 \tValidation Loss: 7.722947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = test_df[0][['image_name']]\n",
        "res['image_name'] = res['image_name'].str.split(\"/\").str[-1]\n",
        "for idx in range(len(test_df)):\n",
        "  test_df[idx] = test_df[idx].sort_values('image_name',ignore_index=True)\n",
        "all = pd.concat(test_df,axis=1)\n",
        "# all.to_csv('drive/MyDrive/DL_Project/raw_predictions.csv', index=False, header=False)\n",
        "all = all.drop(columns='image_name').astype(float)\n",
        "res['output'] = all.mean(axis=1)\n",
        "res.to_csv('drive/MyDrive/DL_Project/predictions.csv', index=False, header=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOzfn1IW6ggh",
        "outputId": "5eba6a05-8b52-40ab-cdf6-2704c9e51edb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = []\n",
        "\n",
        "for split_i in range(0,k):\n",
        "  ## load best val_model \n",
        "  model_conv = CNN1(True).to(device)\n",
        "  model_conv.load_state_dict(torch.load('drive/MyDrive/DL_Project/'+str(split_i)+'model.pth'))\n",
        "  model_conv.eval()\n",
        "  df = pd.DataFrame(columns=['image_name','output'])\n",
        "  for batch_idx, sample_batched in enumerate(test_dataloader):\n",
        "     image= sample_batched['image'].to(device)\n",
        "     img_name= sample_batched['img_name']\n",
        "     output = model_conv(image).type(torch.LongTensor).reshape(-1)\n",
        "     img_name = np.array(img_name).reshape(output.shape[0],1)\n",
        "     o = output.cpu().data.numpy().reshape(output.shape[0],1)\n",
        "     a = np.concatenate((img_name,o),axis=1)\n",
        "     df = df.append(pd.DataFrame(a, columns=df.columns), ignore_index=True)\n",
        "  df.to_csv('drive/MyDrive/DL_Project/' + str(split_i) + '4fcvnotta.csv', index=False, header=False)  \n",
        "  test_df.append(df)"
      ],
      "metadata": {
        "id": "m_0CKzVLTkv6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['output'] = df['output'].astype(float)\n",
        "df['output'].value_counts()\n",
        "df3 = pd.merge(df1,df,how='inner',on='image_name')"
      ],
      "metadata": {
        "id": "dAA-CpXRqd-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "y-2hLN44Q2m3",
        "outputId": "4d70334a-2ebb-440f-9d5a-6045f3cbdc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       image_name output\n",
              "0     drive/MyDrive/DL_Project/Val/Image_0986.png      4\n",
              "1     drive/MyDrive/DL_Project/Val/Image_1157.png      4\n",
              "2     drive/MyDrive/DL_Project/Val/Image_0468.png      4\n",
              "3     drive/MyDrive/DL_Project/Val/Image_0379.png      4\n",
              "4     drive/MyDrive/DL_Project/Val/Image_0239.png      4\n",
              "...                                           ...    ...\n",
              "1296  drive/MyDrive/DL_Project/Val/Image_0574.png      4\n",
              "1297  drive/MyDrive/DL_Project/Val/Image_0856.png      4\n",
              "1298  drive/MyDrive/DL_Project/Val/Image_0571.png      4\n",
              "1299  drive/MyDrive/DL_Project/Val/Image_0011.png      4\n",
              "1300  drive/MyDrive/DL_Project/Val/Image_1041.png      4\n",
              "\n",
              "[1301 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-737cd20f-d537-4e13-b2fb-44837eabe0c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_0986.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_1157.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_0468.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_0379.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_0239.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1296</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_0574.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1297</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_0856.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1298</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_0571.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1299</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_0011.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1300</th>\n",
              "      <td>drive/MyDrive/DL_Project/Val/Image_1041.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1301 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-737cd20f-d537-4e13-b2fb-44837eabe0c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-737cd20f-d537-4e13-b2fb-44837eabe0c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-737cd20f-d537-4e13-b2fb-44837eabe0c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting image name from the image path\n",
        "df['image_name']=df['image_name'].str.split(\"/\").str[-1]"
      ],
      "metadata": {
        "id": "HVRo94hEyBjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('drive/MyDrive/DL_Project/predictions.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "AFib873XyETp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "qDgX-3G5yF56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "24abc625-d372-4812-bb76-ab2007df71b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       image_name output\n",
              "0  Image_1260.png     22\n",
              "1  Image_0917.png      6\n",
              "2  Image_0869.png      5\n",
              "3  Image_0763.png      6\n",
              "4  Image_1156.png     12"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b67e51f9-41fd-4e73-9fca-a39e1f6a35de\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Image_1260.png</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Image_0917.png</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Image_0869.png</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image_0763.png</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Image_1156.png</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b67e51f9-41fd-4e73-9fca-a39e1f6a35de')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b67e51f9-41fd-4e73-9fca-a39e1f6a35de button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b67e51f9-41fd-4e73-9fca-a39e1f6a35de');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}